TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 100  # Reduced from 500
  per_device_train_batch_size: 4  # Increased from 2
  per_device_eval_batch_size: 4   # Increased from 2
  weight_decay: 0.01
  logging_steps: 100  # Reduced logging frequency
  eval_strategy: steps
  eval_steps: 200    # Reduced evaluation frequency
  save_steps: 200    # Reduced save frequency
  gradient_accumulation_steps: 2  # Reduced from 4
  save_total_limit: 2  # Reduced from 3
  load_best_model_at_end: true
  # Added performance optimizations
  fp16: true  # Enable mixed precision training
  dataloader_num_workers: 4  # Enable parallel data loading
  gradient_checkpointing: true  # Save memory
  no_cuda: false  # Use GPU if available